{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaVA Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo images can be found inside `images` folder. There are four images, their labels are {img1: alert, img2:drowsy, img3: alert, img4: drowsy}. Follow upcoming cells to run the demo for all three versions used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, pipeline\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize test images\n",
    "images = [Image.open(\"./images/img1.jpg\"), Image.open(\"./images/img2.jpg\"), \n",
    "          Image.open(\"./images/img3.jpg\"), Image.open(\"./images/img4.jpg\")]\n",
    "\n",
    "# Set quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run your desired variant, simply uncomment one of the model_id's in the cell below. This is the only thing you have to change, the rest of the code remains the same. The available options to use are:\n",
    "- **LLaVA-7B**: `model_id = \"llava-hf/llava-1.5-7b-hf\"`\n",
    "- **LLaVA-13B**: `model_id = \"llava-hf/llava-1.5-13b-hf\"`\n",
    "- **BakLLaVA**: `model_id = \"llava-hf/bakLlava-v1-hf\"`\n",
    "\n",
    "For this demo, we used LLaVA-13B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.10k/1.10k [00:00<00:00, 8.49MB/s]\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Downloading shards: 100%|██████████| 6/6 [00:01<00:00,  4.10it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [04:05<00:00, 40.87s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Set the model ID (uncomment your desired variant)\n",
    "\n",
    "# model_id = \"llava-hf/llava-1.5-7b-hf\" # (1)\n",
    "model_id = \"llava-hf/llava-1.5-13b-hf\" # (2)\n",
    "# model_id = \"llava-hf/bakLlava-v1-hf\" # (3)\n",
    "\n",
    "# Leverage the image-to-text pipeline from transformers\n",
    "pipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"quantization_config\": quantization_config})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the prompt, you need to write it in the format specified in the next cell (we used our fine tuned prompt in this demo). The output of the LLM is the text coming after \"ASSISTANT: \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[img1]\n",
      "\n",
      "USER:  \n",
      "Carefully examining the driver's current state, is this driver fully alert and very engaged in safe driving practices? Answer only with 'yes' or 'no'.\n",
      "ASSISTANT: Yes\n",
      "\n",
      "----------------------------------------------\n",
      "[img2]\n",
      "\n",
      "USER:  \n",
      "Carefully examining the driver's current state, is this driver fully alert and very engaged in safe driving practices? Answer only with 'yes' or 'no'.\n",
      "ASSISTANT: No\n",
      "\n",
      "----------------------------------------------\n",
      "[img3]\n",
      "\n",
      "USER:  \n",
      "Carefully examining the driver's current state, is this driver fully alert and very engaged in safe driving practices? Answer only with 'yes' or 'no'.\n",
      "ASSISTANT: Yes\n",
      "\n",
      "----------------------------------------------\n",
      "[img4]\n",
      "\n",
      "USER:  \n",
      "Carefully examining the driver's current state, is this driver fully alert and very engaged in safe driving practices? Answer only with 'yes' or 'no'.\n",
      "ASSISTANT: No\n",
      "\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Write the prompt, should be in the format --> USER: <image>\\n<prompt>\\nASSISTANT:\n",
    "# Note that for this prompt, an answer of 'yes' means alert and 'no' means drowsy.\n",
    "prompt = \"USER: <image>\\nCarefully examining the driver's current state, is this driver fully alert and very engaged in safe driving practices? Answer only with 'yes' or 'no'.\\nASSISTANT:\"\n",
    "\n",
    "# Get and display predictions (for this demo, all predictions are correct)\n",
    "i = 1\n",
    "for img in images:\n",
    "    output = pipe(img, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
    "    print(f\"[img{i}]\\n\")\n",
    "    print(output[0][\"generated_text\"])\n",
    "    print(\"\\n----------------------------------------------\")\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
