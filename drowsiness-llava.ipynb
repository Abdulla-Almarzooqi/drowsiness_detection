{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to instructions stated in `llava_demo.ipynb` (under `Demo` folder) on how to use the different LLaVA variants. Read the comments in the code for further guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdulla.almarzooqi/miniconda3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdulla.almarzooqi/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [01:55<00:00, 19.24s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Set the model ID, choose your variant (as stated in the demo)\n",
    "\n",
    "# model_id = \"llava-hf/llava-1.5-7b-hf\" # (1)\n",
    "model_id = \"llava-hf/llava-1.5-13b-hf\" # (2)\n",
    "# model_id = \"llava-hf/bakLlava-v1-hf\" # (3)\n",
    "\n",
    "# Leverage the image-to-text pipeline from transformers\n",
    "pipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"quantization_config\": quantization_config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the list of ground truth labels\n",
    "# You just have to pass the path to the true labels along with the path to the corresponding\n",
    "# test images.\n",
    "\n",
    "def get_ground_truth_labels(labels_folder, image_files):\n",
    "    ground_truth_labels = []\n",
    "\n",
    "    for image_file in image_files:\n",
    "        # Get the corresponding label file\n",
    "        label_file_path = os.path.join(labels_folder, os.path.splitext(image_file)[0] + '.txt')\n",
    "\n",
    "        # Read the first line of the label file to get the ground truth class\n",
    "        with open(label_file_path, 'r') as label_file:\n",
    "            ground_truth = int(label_file.readline().split()[0].strip())\n",
    "            ground_truth_labels.append(ground_truth)\n",
    "\n",
    "    return ground_truth_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust paths accordingly\n",
    "labels_folder_path = '/home/abdulla.almarzooqi/Desktop/AI702Project/Drowsiness-/-Fatigue_Detection-4/test/labels'\n",
    "images_folder = '/home/abdulla.almarzooqi/Desktop/AI702Project/Drowsiness-/-Fatigue_Detection-4/test/images'\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [f for f in os.listdir(images_folder)]\n",
    "\n",
    "# Get corresponding list of ground truths\n",
    "ground_truth_labels = get_ground_truth_labels(labels_folder_path, image_files)\n",
    "\n",
    "# List of images\n",
    "images = [Image.open(os.path.join(images_folder, image_file)) for image_file in image_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note the comments about the prompt format and the if-else statement below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "# Write the prompt, should be in the format --> USER: <image>\\n<prompt>\\nASSISTANT:\n",
    "prompt = \"USER: <image>\\nCarefully examining the driver's current state, is this driver fully alert and very engaged in safe driving practices? Answer only with 'yes' or 'no'.\\nASSISTANT:\"\n",
    "\n",
    "# Iterate to get predictions\n",
    "predictions = []\n",
    "for image in images:\n",
    "\n",
    "    output = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
    "    answer = output[0][\"generated_text\"].split(\":\")[-1].strip().lower() # Extract the answer\n",
    "\n",
    "    # IMPORTANT: Based on your prompt, if yes means alert, then set prediction = 0 for the case\n",
    "    #            answer == 'yes', otherwise set prediction = 1\n",
    "    if answer == 'yes':\n",
    "        prediction = 0 # For our prompt, 'yes' means alert...\n",
    "    else:\n",
    "        prediction = 1 # ... and 'no' means drowsy\n",
    "\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       alert       0.87      0.81      0.84        58\n",
      "      drowsy       0.97      0.98      0.98       424\n",
      "\n",
      "    accuracy                           0.96       482\n",
      "   macro avg       0.92      0.90      0.91       482\n",
      "weighted avg       0.96      0.96      0.96       482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the evaluation metrics\n",
    "\n",
    "print(classification_report(ground_truth_labels, predictions, labels=[0,1], target_names=['alert', 'drowsy']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
